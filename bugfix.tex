Corrective changes such as bug fixes are frequently applied by developers to eliminate defects in software. There are mainly two lines of research conducted: (1) empirical studies to characterize bugs and corresponding fixes, 
%~\cite{Fenton2000:QAF,Li2006:TCE,Kim2006:MBF,Lu2008:LMC,Nguyen2010:RBF,Yin2011:FBB,Park2012:supplementary,Zhong2015:ESR}
 and (2) automatic approaches to detect and fix such bugs. 
% ~\cite{Engler2000:CSR,Bush2000:SAF,Hangal2002:TDS,Hovemeyer2004:FBE,Naik2006:ESR,Weimer2009:AFP}. 
 There is no clear boundary between the two lines of research, because some prior projects first make observations about particular kinds of bug fixes empirically and then subsequently leverage their observed characteristics to find more bugs and fix them. Below, we discuss a few representative examples of empirical studies with such flavor of characterizing existing bugs and fixing them.   
% ~\cite{Li2006:CPMiner,Pham2010:DRS,Jin2012:UDR,Kim2013:PAR} 

 
\subsubsection{Empirical Studies of Bug Fixes.}
In this section, we discuss two representative studies on bug fixes. These studies are not the earliest, seminal works in this domain. Rather, the flavor and style of their empirical studies is representative. Li et al. conducted is a large scale characterization of bugs by digging through bug reports in the wild and by quantifying the extent of each bug type~\cite{Li2006:TCE}. S. Kim et al.'s {\em memory of bug fixes}~\cite{Kim2006:MBF} uses fine-grained bug fix histories to measure the extent of recurring, similar bug fixes and to assess the potential of automating similar fixes based on change history.  

% write studies with past tense 
Li et al.~conducted an empirical study of bugs from two popular open source projects: Mozilla and Apache HTTP Server~\cite{Li2006:TCE}. By manually examining 264 bug reports from the Mozilla Bugzilla database, and 209 bug reports from the Apache Bugzilla database, they investigated the root cause, impact, and software components of each software error that exhibited abnormal runtime behaviors. They observed three major root causes: {\em memory}, {\em concurrency}, and {\em semantics}. The memory bugs accounted for 16.3\% in Mozilla and 12.2\% in Apache. Among memory bugs, NULL pointer dereference was observed as a major cause, accounting for 37.2\% in Mozilla and 41.7\% in Apache. More importantly, semantic bugs were observed to be dominant, accounting for 81.1\% in Mozilla and 86.7\% in Apache. One possible reason is that most semantic bugs are specific to applications. A developer could easily introduce semantic bugs while coding, due to a lack of thorough understanding of software and its requirements. It is challenging to automatically detect or fix such semantic bugs, because diagnosing and resolving them may require a lot of domain-specific knowledge and such knowledge is inherently not generalizable across different systems and applications.   

To understand the characteristics and frequency of project-specific bug fixes, Kim et al.~conducted an empirical study on the bug fix history of five open source projects: ArgoUML, Columba, Eclipse, jEdit, and Scarab~\cite{Kim2006:MBF}. With keywords like ``Fixed'' or ``Bugs'', they retrieved code commits in software version history that are relevant to bug fixes, chopped each commit into contiguous code change blocks (i.e., hunks), and then clustered similar code changes. They observed that 19.3 to 40.3\% bugs appeared repeatedly in version history, while 7.9 to 15.5\% of bug-and-fix pairs appeared more than once. The results demonstrated that project-specific bug fix patterns occur frequently enough and for each bug-and-fix pair, it is possible to both detect similar bugs and provide fix suggestions. The study also showed history-based bug detection could be complementary to static analysis-based bug detection\textemdash the bugs that can be detected by past bug fix histories do not overlap with the bugs that can be detected by a static bug finding tool, PMD~\cite{PMD}. 

\todo{Miryung done up to here} 

\subsubsection{Rule-based Bug Detection and Fixing Approaches.}
This section provides two representative works on rule-based bug detection based on empirical observations.
Engler et al.~define a meta-language for users to easily specify temporal system rules such as ``release locks after acquiring them''~\cite{Engler2000:CSR}. They also extend a compiler to interpret the rules and dynamically generate additional checks in the compiler. If any code snippet violates the specified rule(s), the approach reports the snippet as a software bug. Table~\ref{tab:rule} presents some exemplar system rule templates and instances. With this approach, developers can flexibly define their own rules to avoid some project-specific bugs, without worrying how to implement checkers to enforce the rules. Engler et al.'s later work tailors rule templates to a specific system and checks for contradictions and violations\cite{engler01bugs}.  

\begin{table}[]
\centering
\caption{Sample system rule templates and examples from~\cite{Engler2000:CSR}}
\label{tab:rule}
\begin{tabular}{l|l}
\toprule
Rule template                  & Example                                                 \\ \hline
``Never/always do X''          & ``Do not use floating point in the kernel''             \\\hline
``Do X rather than Y''         & ``Use memory mapped I/O rather than copying''           \\ \hline
``Always do X before/after Y'' & ``Check user pointers before using them in the kernel''\\
\bottomrule
\end{tabular}
\end{table} 

As another example of rule-based bug detection is CP-Miner, an automatic approach to find copy-paste bugs in large-scale software~\cite{Li2006:CPMiner}. CP-Miner is motivated by Chou et al.'s finding that, under the Linux {\sf drivers/i2o} directory, 34 out of 35 errors were caused by copy-paste~\cite{Chou2001:ESO} and based on the insight that when developers copy and paste, they may forget to consistently rename identifiers. CP-Miner first identifies copy-paste code in a scalable way, and then detects bugs by checking for such specific rules, for example, consistent renaming of identifiers. %Many previously unknown bugs in popular operating systems were detected in this way, 49 in Linux and 31 in FreeBSD, meaning that CP-Miner can effectively capture copy-paste related bugs. 
%Similarly, FixWizard identifies code clones based on object usage and interactions, recognizes recurring bug-fixes to the clones, and suggests a location and example edit~\cite{Nguyen2010:RBF}. However, it does not generate fixes.
\subsubsection{Automated Repair.} 
Automatic program repair generates candidate patches and checks correctness using compilation, testing, and/or specification. 

One set of techniques uses {\em search-based repair}~\cite{harman07} or predefined repair templates to generate many candidate repairs for a bug, and then validates them using indicative workloads or test suites~\cite{Kim2013:PAR, genprog-icse2012, Perkins09:clearview}. For example, GenProg generates candidate patches by replicating, mutating, or deleting code \emph{randomly} from the existing program. Many of these approaches can scale to repair defects in large systems with human-competitive costs. However, they tend to find the smallest possible fix for a given failure, and current evidence suggests that humans may find the resulting patches unacceptable in many cases~\cite{genprog-maintainability,Kim2013:PAR}. 

Another class of strategies in automatic software repair relies on {\em specifications} or {\em contracts} to guide sound patch generation~\cite{gopinath2011, liblit2011, liu2012, semfix13,Wei:2010:AutoFix-E}. This provides confidence that the output is correct. For example, AutoFix-E~\cite{Wei:2010:AutoFix-E} generates simple bug fixes from manually prescribed contracts. Genetic programming has also been used to co-evolve defect repairs and unit test cases~\cite{Arcuri11,wilkerson2012}; these techniques tend to rely at least in part on formal specifications to define correctness~\cite{arcuriy08,wilkerson11}.  Such techniques struggle to scale, and are usually limited to manually specified code, which is rare in practice.

Some approaches are specialized for particular types of bugs only. For example, FixMeUp inserts missing security checks inter-procedurally using a specification, but these additions are very specific and stylized~\cite{son2013fix}. Given concurrency error reports, Jin et al.~select from and test a handful of synchronization patterns to fix them~\cite{JZDLL:12} and insert appropriate synchronization into a compiler intermediate representation. PAR~\cite{Kim2013:PAR} encodes ten common bug fix patterns from Eclipse JDT's version history to improve the patch suggestions of GenProg~\cite{Weimer2009:AFP}. However, the patterns are created manually. 

